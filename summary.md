# ผลลัพธ์การสร้างเกม Frogger และ Unit Test ด้วย Generative AI
## จากการประเมินประสิทธิภาพของ Generative AI ทั้ง 3 โมเดล ในการสร้างเกม Frogger ด้วย Pygame และ Unit Test ด้วย Pytest ได้ผลลัพธ์ดังนี้
1. Gemini 2.5 Pro 
game: สามารถสร้างโค้ดที่ทำงานได้ตั้งแต่ครั้งแรก แต่พบข้อผิดพลาด คือขอนไม้ในช่องสุดท้ายไม่ปรากฏขึ้น ทำให้ผู้เล่นไม่สามารถพากบไปถึงเป้าหมายได้ นอกจากนี้ User Interface (UI) ยังขาดความชัดเจนในการแบ่งโซนและไม่ได้ระบุตำแหน่งของ "บ้าน" ที่เป็นเป้าหมาย

Unit Test: สามารถสร้าง Unit Test ที่รันได้ทันที แต่ผลการทดสอบ ไม่ผ่านทั้งหมด โดยแม้จะกำหนดเงื่อนไข statement coverage = 100% ใน Prompt แต่ผลลัพธ์ที่ได้จริงอยู่ที่ 77%
2. Claude Sonet 4.5
game: ทำงานได้ในครั้งแรก แต่พบปัญหาด้าน Layout ที่จัดวางองค์ประกอบไม่ถูกต้อง ซึ่งอาจสร้างความสับสนให้ผู้เล่นได้ และ UI ขาดความชัดเจนในการสื่อความหมาย

Unit Test: สร้าง Unit Test ที่รันได้ แต่ผลการทดสอบ ไม่ผ่านทั้งหมด โดยผลลัพธ์ Statement Coverage ที่วัดได้จริงคือ 90%

3. ChatGPT Model 5 
game: สร้างโค้ดเกมที่ทำงานได้สมบูรณ์ตั้งแต่ครั้งแรก และมี คุณภาพสูงกว่าอีกสองโมเดล โดยมี Layout ที่เป็นระเบียบชัดเจนและกลไกของเกมทำงานครบถ้วนตามข้อกำหนด แต่ขาดการระบุตำแหน่งของ "บ้าน" ที่เป็นเป้าหมายเหมือน Gemini 2.5 Pro

Unit Test: สร้าง Unit Test ที่รันได้ โดยสามารถรันได้และผ่านการทดสอบทั้งหมด (All tests passed) อย่างไรก็ตาม ผลการทดสอบไม่ได้เป็นตัวกำหนดว่า software ที่สร้างนั้นมีคุณภาพดีไหม แต่ต้องดูองค์ประกอบโดยรวมที่ test ด้วยว่าสอดคล้องจริงไหม และ Statement Coverage ที่ได้จริงอยู่ที่ 79% ซึ่งยังไม่ถึงเป้าหมายที่กำหนด

## จากการใช้เครื่องมือ Static Analysis เพื่อเปรียบเทียบค่า Complexity และ Maintainability พบว่า:
Complexity: โค้ดจาก AI ทั้งสามโมเดลมีค่า Complexity อยู่ในระดับ A ทั้งในส่วนของโค้ดเกมและ Unit Test ซึ่งถือว่ามีคุณภาพดีและไม่ซับซ้อน
Maintainability Index: พบความแตกต่างอย่างมีนัยสำคัญ
ChatGPT และ Gemini: มีคะแนนใกล้เคียงกันที่ประมาณ 40/100
Claude: มีคะแนนต่ำเพียง 1/100 ซึ่งบ่งชี้ว่าโค้ดที่สร้างขึ้นมานั้นดูแลรักษาและแก้ไขได้ยากมาก ซึ่งจำนวนบรรทัดโค้ดที่สร้างมานั้น Claude มีจำนวนบรรทัดเยอะสุด และมากกว่าตัวอื่นค่อนข้างเยอะ 

## สรุปผลการทดลอง 
AI สามารถช่วยในการบวนการ coding และ testing ได้จริง แต่ยังคงอยู่ในกรอบที่ว่าเป็น "ผู้ช่วย" มากกว่า "ผู้สร้างที่สมบูรณ์แบบ" ประสิทธิภาพของ AI จะใช้ได้สูงสุดอยู่ที่ผู้ใช้มีความรู้ความเข้าใจในการตรวจสอบและแก้ไขผลลัพธ์ที่ได้ การใช้งาน AI อย่างถูกวิธีและมีประสิทธิภาพจะช่วยลดระยะเวลาในการพัฒนาได้ แต่หากขาดความเข้าใจ อาจทำให้เสียเวลาในการทำงานเพิ่มมากขึ้น

## ปัญหาที่พบ
- การออกแบบ Prompt นั้นต้องมีความเจาะจงและชัดเจนเพียงพอในสิ่งที่ต้องการ เพื่อให้ AI เข้าใจบริบทและเงื่อนไขที่ซับซ้อนได้อย่างครบถ้วน
- ความน่าเชื่อถือของผลลัพธ์ AI ยังมีโอกาสสร้างผลลัพธ์ที่ไม่ได้ตรงตามความต้องการ หรือไม่เป็นไปตามเงื่อนไขที่บังคับไว้ เช่น กรณีที่ AI ทุกตัวไม่สามารถสร้าง Unit Test ที่มี statement coverage ถึง 100% ได้สำเร็จ แม้จะมีการระบุข้อกำหนดไว้ใน Prompt อย่างชัดเจน
- การควบคุมเวอร์ชั่น โดยจำเป็นต้องมีการระบุเวอร์ชั่นของภาษา และไลบรารีที่ใช้งานใน prompt ตั้งแต่แรก เพื่อป้องกันปัญหาความเข้ากันได้ของโค้ดที่ AI สร้างขึ้น 
